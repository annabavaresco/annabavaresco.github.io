---
title:          "Testing Spatial Intuitions of Humans and Large Language and Multimodal Models in Analogies?"
date:           2025-04-04 00:01:00 +0800
selected:       true
pub:            "Proceedings of the 2nd Workshop on Analogical Abstraction in Cognition, Perception, and Language (Analogy-Angle II)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-custom badge-success">Spotlight</span>'
pub_date:       "2025"

abstract: >-
  Language and Vision-Language Models exhibit impressive language capabilities akin to human reasoning. However, unlike humans who acquire language through embodied, interactive experiences, these models learn from static datasets without real-world interaction. This difference raises questions about how they conceptualize abstract notions and whether their reasoning aligns with human cognition. We investigate spatial conceptualizations of LLMs and VLMs by conducting analogy prompting studies with LLMs, VLMs, and human participants. We assess their ability to generate and interpret analogies for spatial concepts. We quantitatively compare the analogies produced by each group, examining the impact of multimodal inputs and reasoning mechanisms. Our findings indicate that generative models can produce and interpret analogies but differ significantly from human reasoning in their abstraction of spatial concepts - variability influenced by input modality, model size, and prompting methods, with analogy-based prompts not consistently enhancing alignment. Contributions include a methodology for probing generative models through analogies; a comparative analysis of analogical reasoning among models, and humans; and insights into the effect of multimodal inputs on reasoning.
# cover:          assets/images/covers/cover3.jpg
authors:
  - Ivo Bueno
  - Anna Bavaresco
  - Jo√£o Miguel Cunha
  - Philipp Wicke
links:

  Paper: https://aclanthology.org/2025.analogyangle-1.9/
  Code: https://github.com/cisnlp/spatial_intuitions
---
